two algorithm design techniques are discussed
	the incremental method (like insertion sort)
	the divide-and-conquer method (DaC for short)
		DaC in different cases results in faster sorting and simpler analysis of
			... run times

***2.3.1 the divide-and-conquer method***

recursive algos often use DaC
	that is the break problems down into simpler smaller problems
		...then solve them recursively
			...then combine the solutions all together
				base cases are solved directly
					and more large or complex cases are solved with:
						**divide, conquer, combine**
						
merge sort is an example of an array that divides an array up into halves
	... recursively, sorts them, and then merges the results
		the base case is when all sub-arrays of the input have a length of one
			...and this is because an array of length 1 is sorted
			
the merge step in merge sort combines two sorted sub-arrays into one sorted
	...sub-array using the merge procedure
		it assumes the left and right arrays are sorted
			and replaces the specified range with the sorted items
			
merging two sorted piles of cards involves comparing the top cards of each pile
	each pile has `n = 2` cards, and the number of steps required is at least `n`/2`
		...steps and at most `n-1` steps
			one pile will be empty after `n-1` steps
				since each steps takes a constant amount of time
					the total time for merging is proportional to n, or `O(n)`
					
the merging procedure copies two sub-arrays
	...left, and right, into temporary sub-arrays
		then merging them back into the range left and right corresponds to
			1 lines 1 and 2 compute the lengths

```rust
fn merge(arr: &mut [usize], p: usize, q: usize, r: usize) {
	let n_l = q - p + 1; // length of A[p..q]
	let n_r = r - q; // length of A[q+1..r]
	let mut l = Vec::with_capacity(n_l);
	let mut r = Vec::with_capacity(n_r);
	for i in 0..n_l {
		l[i] = arr[p + i];
	}
	for j in 0..n_r {
		r[j] = arr[q + 1 + j];
	}
		let mut i = 0; // i indexes the smallest remaining element in L
		let mut j = 0; // j indexes the smallest remaining element in R
		let mut k = p; // k indexes the location in A to fill
	while i < n_l && j < n_r {
		if l[i] <= r[j] {
			arr[k] = l[i];
			i += 1;
		} else {
			arr[k] = r[j];
			j += 1;
		}
		k += 1;
	}
	while i < n_l {
		arr[k] = l[i];
		i += 1;
		k += 1;
	}
	while j < n_r {
		arr[k] = r[j];
		j += 1;
		k += 1;
	}
}
```

first, the sub array lengths are set to n_l and n_r
	then, two vectors are allocated from the provided vector
		... following the pseudocode, two for loops assign the vectors as well (silly)
			i, j, and k are all initialized as 0
				a while loop sees that if i and j are less than their respective n value
					... then it assigns the lower of the two to the next position in arr
						after this two while loops are run to insert the remaining
							...values since a few may be left for left or right vectors

	with consideration of copyright I've decided to not directly post screen
		shots of diagrams from the book directly into my notes...
			it's academic material but I don't have permission from the
				...creators of this book to use their hard work like that
					I will however make notes on them and note the figure #'s

**figure 2.3**
	the diagram illustrates the process of merging a couple arrays L and R
		the diagram is an illustration of the rust code above
			... in the case of using merge sort
				1 - the setup
					the arrays are sorted into two arrays
						and i j k are tracking the current positions
				2 - step by step
					in the loops values are copied from the two paritions
						... into the array provided to the merge fn
				3 - iterative process
					the diag shows the process of merging step by step
						at each stage, a gets filled with the smallest remaining
							... elements from L and R
								once merged the array is sorted

the merge function runs in `O(n)` time since:
	constant time for operations in the ram model
		the for loops take up constant time for all of their operations
			the inits of vars all take constant time
				the while loops take up constant time since each item is copied only once

when used in merge-sort the merge proc is invoked recursively
	the algo works like...
		divide: take the initial array and split it up
		conquer: split each sub array up again and again until only 1 remains
		combine: the merge procedure merges the sorted sub arrays back into the
			...array provided to merge_call

the overall complexity of merge is O(n log (n)) because at each level of recursion
	the merging steps take O(n) time, and there are log(n) steps due to the
		halving of the array for each step into merge_call

***2.3.2 analyzing DaC algorithms***

	should probably start tagging my sections up as I go
		...and go back and tag up the ones before this
			if something is about DaC then it should get the #DaC tag
				then I'll have a really nice node graph in the end c:
			
#DaC

the run time of T(n) of a DaC algo is described be a _recurrence

	T(n) = {
		O(1)               if n < n0
		C(n) + aT(n/b)​     if n ≥ n0​​
	}
	
base case, `if n < n_0`, then the algo will take constant time, or `O(1)`
	for others,
		divide: split into sub problems `n/b`, where b is # of problems
		conquer: solving each sub problem takes `T(n/b)`, so solving all is
			... `a * T(n/b)
		combine: merging the solution takes C(n)
	for merge, `a = 2, b = 2, C(n) = O(n)` leads to the recurrence
		... `T(n) = O(n) + 2T(n/2)
			solving this gives time complexity of `O(n log (n))`

in DaC algos like merge sort - the size of the subprobs are approx
	`n/2` even if they involve ceil and floor funcs
		... the difference between them is small enough to ignore
			... and this leads to simpler recurrence expressions

in recurrences later the base cases of `T(n) = O(1)` will be dropped because
		...are _usually_ constant time (for small inputs where `n < n0`)
			this will simplify recurrence equations for later

***analysis of merge sort***

the recurrence for the worst run time of `T(n)` would be
	divide: the divide step takes constant time, `D(n) = O(1)`
	conquer: using recursion to solve sub problems of `n/2` size which
		... will add `2T(n/2)` to the run time
	combine: which will add `C(n) = O(n)`
		the total recurrence,
			`T(n) = 2T(n/2) + O(n)`

merge sorts run time is `O(n log(n))`  where insertion sort is `O(n^2)`
	this would mean that for a values of n merge sort will have a more efficient
		...run time since it scales more slowly
			for larger inputs of n merge sort is best

the recur of merge sort can be understood without 'the master theorem' - to keep it simple
	... assuming n is a power of 2, and the base case is T(1) = c_1
		... the recur shows that the time for a prob of size n
			... depends on solving two subprobs of size n/2
				... with a cost proportional to n
					this results in a solution of T(n) = O(n log(n))

_figure 2.5 is provided - showing a tree that describes run time cost of each step_

the recur for merge sort can be gathered from the tree
	the process involves expanding each step into a node
		...where each node represents a sub problem
			breakdown:
				1, divide, the cost of dividing the problem at the top level `c_2 * n`
				2, conquer, at each level, the # doubles, but the cost per node is divided
					...in half
				3, combine, each level contributes the same total cost, `c_2 * n`
					... until reaching the base case where each arr is of size 1
						... and the total cost for the leaf level is `c_1 * n`

for recursion tree has `log(n) + 1` levels --> each level above
	... the leaves has a cost of `c_2 * n`, and each leaf contributes `c_1 * n`
		the total cost is `c_2 * n * log(n)`, which is `O(n log n)`