














2.3 - D E S I G N I N G A L G O R I T H M S

you can choose from many algo design techniques - insertion sort is considered the incremental method

... for each element A[i] insert it into it's proper place in the subarray a[0..i-1]

this section examines another method known as 'divide and conq' DaC for short which will be explain even more in chapter 4...

we'll use this method to desig na sort algo with a worst case run time is much better than insertion sort's by comparison

one advantage of using an algo that follows DaC is analyzing it's run time is often straight forward..

using techniques we'll explore in ch 4

2.3.1 the dac method

many useful algos are recursive in structure: to solve a prolem they call themselves one or more times to handle closely

...related sublems... these algos typically follow the daq method; they break the problem into several subproblems that are si-

...milar to the original problem but smaller in size - solve the subproblems recursively, then combine the results

... to create a solution to the original problem

in the daq method if the problem is small enough - the base case - you just solve it without recursion... else the recursive case

there are three steps...

div: the problem into one or more subproblems that are smaller versions of the same problem

conq: solve the subproblems recursively

combine: combine the solutions to form a solution to the original problem

mege sort follows the dac method

in each step it sorts a[p:r] starting with the entire array a[1:n] and recurses down to smaller and smaller subarrays

here is how merge sort operates...

div: divide subarray a[p:r] to be sorted into two arrays - each half of it's size

compute the midpoint Q of a[p:r] taking the average of p and r and divide into a[p:q] and a[q+1:r]

conq: sort the two subarrays a[p:q] and a[q+1:r] recursively using merge sort

combine: merge the two sorted subarrays ... a and b back into a[p:r]

the recurse bottoms out when the base case a[p:r] has only a single element; p is equal to r; as noted in sert-sort's loop

..invariant an array of only a single index is always sorted

the key op of the merge sort algo occurs in the combination step which merges to adjacent sorted subarrays

.... the merge op is performed by the aix procedure MERGE(a, p, q, r) on the following page...

... where A is the array and p q r are indexes into the arr such that p <= q <= r....

the procedure assumes that the adj subarrs a and b were already sorted -

it merges the two sorted subarrs to form a single sorted subarr that replaces subarr...

.... a[p:r]

the understand how merge proc works lets go back to the cards...

suppose you have two piles face up

... each pile is sorted with smallest values on top

you want ot merge the two piles into a single pile which will be face down

the basic step consists of choosing the smaller of the two on top of the face up piles...

...removing from it's pile when exposes a new top card and placing it face down

repeat this until one input pile is empty

at which time you just take the remaining pile and flip the entire pile

placing it face down on the output pile

how long would this take? each step takes constant time - since comparing just 2 cards - each

  

//not going over the verbose description of merge-sort; instead I just wrote the functions in rust for a pure understanding of the algo

//instead of depending on reading comprehension to kind of get it

  

2.3.2 A N A L Y Z I NG D I V I D E - A N D - C O N Q U E R A L G O R I T H M S

  

when algos contain recursing call their run times can be described by a recurrence equation or recurrence... which often

describes the overall running time on a problem of size n in terms of the running time of the same algorithm on smaller

inputs..! mathematical tools can be used to solve the recurrence and provide bounds on the performance of the algo

a recurrence for the run time of a dac algo falls out from the three steps of the basic method

as we did for insertion sort let T(n) be the worst case run time on a problem of n size.... if the problem size is small enough

say n < n0 for some constant n0 < 0... the straight forward soluton takes constant time... which we write as O(1).... suppose

that the division of the problem yields A subproblems.... each with size n/B, that is, 1/B - the size of the original -

for merge sort both a and b are 2, but we'll see ohter dac's in which they aren't equal - it takes T(n/b) time to solve one

subproblem of n/b... and so it'd take aT(n/b) time to solve all A of the problems... if it'd take D(n) time to divide

the problem into subproblems... and C(n) time to combine the solutions to the sub problems into one solution

to the originial problem... we get the recurrence... a figure is shown visualizing the dac method

in merge-sort

... dividing a length of 8 3 times into sub problems of 2, then 4, then 8 all of 1 item vectors...

then immediately following division is the merging step in equal quantities

figure 2.4 the op of merge sort on A with length 8 that contains a sequence - p, q, r all appear in each subarray

above their values... numbers in italics indicate the order in which m-s and m proc's are called following

the initial call of merge-sort

  

a fomula is shown for the calculation of merge sort

T(n) = O(1) if n< n0, otherwise D(n) + aT(n/b) + C(n)

ch 4 will show how to solve recurrences of this form

  

sometimes the n/b size of the div step isn't an int - for ex; m-s proc divs problem of size n into subprobs of sizes n/2 and n/2, since the diff between them is

at most 1... which for large n is much smaller than the effect of div n by 2 - we'll adjust and say they are both n/2 because it hardly matters in this context

as ch 4 will discuss; this simplification of ignoring floors and ceils does not generally affect the OoG of a solution to dac recurrence

another convention we'll adopt is to omit a statment of base cases of recurrence... which we'll also get into in ch 4... the reason is that base cases are almost always

T(n) = O(1) if n < n0... for some constant n0 > 0 - that's because the run time of an algo on an input of constant size is constant; we save lots of extra writing

by adopting this convention to save time!

analyzing merge sort

here's how to set up the recurrence for T(n); the worst case run time is merge sort on n numbers...

Div: the div step computers the middle of the sub, which is in constant time therefore D(n) = O(1)

Conq: recursively solvin two subprobs, each of size n/2, contributes 2T(n/2) ignoring ceils and flooors

Comb: since th merge proc on n-element sub-arrays takes O(n) time we have c(n) = O(n)

when adding the funcs D(n) and C(n) for the merge sor tanalysis, we are adding a function that's O(n) and a function that is O(1) - this sum is a linear func of n...

that is it is roughly... proportional to n when n is large, and so merge sort's dividing and comining times together are O(n)... add O(n) to 2T(n/2) term from the conq

gives a recurrence for the worse case run time T(n) of merge sort

T(n) = 2T(n/2) + O(n)

ch 4. presents the master theorem - which shows that T(n) = O(n lg n)

compared with insertion sort whose worst case run time is O(n^2) merge sort trades away a factor of n for a factor of lg n... because the log func grows more slowly

any linear func that's a good trade.... for large enough inputs... merge sort with it's O(n lg n) worst case running time, outperforms insertion sort, whose

worst-case running time is O(n^2)

master theorum isn't required to understand why the solution to recurrence 2.3 is T(n) = O(n lg n); for simplicity assume n is an exact power of 2, and that the implicit base

case is n = 1... then recurrence (2.3) is essentially... constant time if n = 1, or 2T(n/2) + cn if n < 1

where c1 > 0 represents the time required to solve a problem of size 1, and c2 > 0 is the time per array element of the div and comb steps...

figure illustrated one way of figuring out the solution to recurrenec (2.4)